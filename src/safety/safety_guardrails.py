"""
Safety Guardrails for Domain Name Generation

This module implements content filtering and safety measures to prevent inappropriate domain name generation.
"""

import os
import json
import re
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import yaml
from datetime import datetime
import hashlib

@dataclass
class SafetyCheck:
    """Represents a safety check result."""
    check_id: str
    check_type: str
    input_text: str
    is_safe: bool
    risk_score: float
    flagged_keywords: List[str]
    reason: str
    timestamp: str

@dataclass
class SafetyViolation:
    """Represents a safety violation."""
    violation_id: str
    violation_type: str
    input_text: str
    generated_output: str
    severity: str
    flagged_content: List[str]
    business_type: str
    timestamp: str

class SafetyGuardrails:
    """Safety guardrails for domain name generation."""
    
    def __init__(self, config_path: str = "config/model_config.yaml"):
        """Initialize safety guardrails."""
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        # Load safety configuration
        self.inappropriate_keywords = self.config['safety']['inappropriate_keywords']
        self.restricted_business_types = self.config['safety']['restricted_business_types']
        self.moderation_threshold = self.config['safety']['moderation_threshold']
        self.rejection_message = self.config['safety']['rejection_message']
        
        # Initialize safety patterns
        self.safety_patterns = self._initialize_safety_patterns()
        
    def _initialize_safety_patterns(self) -> Dict[str, List[str]]:
        """Initialize safety patterns for different types of violations."""
        return {
            "hate_speech": [
                r"hate", r"racist", r"discriminat", r"supremacist", r"nazi", r"fascist",
                r"white\s*power", r"black\s*power", r"extremist", r"terrorist"
            ],
            "violence": [
                r"kill", r"murder", r"assassin", r"bomb", r"explosive", r"weapon",
                r"gun", r"shoot", r"attack", r"violence", r"blood", r"death"
            ],
            "illegal_activities": [
                r"illegal", r"criminal", r"fraud", r"scam", r"phishing", r"malware",
                r"virus", r"hack", r"steal", r"drug", r"gambling", r"porn"
            ],
            "spam": [
                r"spam", r"scam", r"fake", r"clickbait", r"adware", r"popup",
                r"free\s*money", r"get\s*rich", r"make\s*money\s*fast"
            ],
            "trademark_violations": [
                r"google", r"facebook", r"amazon", r"apple", r"microsoft", r"netflix",
                r"youtube", r"twitter", r"instagram", r"linkedin", r"uber", r"airbnb"
            ],
            "inappropriate_content": [
                r"sex", r"porn", r"adult", r"nude", r"explicit", r"inappropriate",
                r"offensive", r"vulgar", r"profanity", r"curse"
            ]
        }
    
    def check_input_safety(self, prompt: str, business_type: str = None) -> SafetyCheck:
        """Check if input prompt is safe for domain name generation."""
        check_id = hashlib.md5(f"{prompt}_{datetime.now()}".encode()).hexdigest()[:8]
        
        # Check for inappropriate keywords
        flagged_keywords = []
        risk_score = 0.0
        
        prompt_lower = prompt.lower()
        
        # Check against inappropriate keywords
        for keyword in self.inappropriate_keywords:
            if keyword.lower() in prompt_lower:
                flagged_keywords.append(keyword)
                risk_score += 0.3
        
        # Check against safety patterns
        for pattern_type, patterns in self.safety_patterns.items():
            for pattern in patterns:
                if re.search(pattern, prompt_lower, re.IGNORECASE):
                    flagged_keywords.append(pattern_type)
                    risk_score += 0.4
        
        # Check business type restrictions
        if business_type and business_type in self.restricted_business_types:
            flagged_keywords.append(f"restricted_business_type: {business_type}")
            risk_score += 0.5
        
        # Determine if input is safe
        is_safe = risk_score < self.moderation_threshold
        
        # Generate reason
        if not is_safe:
            reason = f"Input flagged with risk score {risk_score:.2f} due to: {', '.join(flagged_keywords)}"
        else:
            reason = "Input passed safety checks"
        
        return SafetyCheck(
            check_id=check_id,
            check_type="input_safety",
            input_text=prompt,
            is_safe=is_safe,
            risk_score=risk_score,
            flagged_keywords=flagged_keywords,
            reason=reason,
            timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        )
    
    def check_output_safety(self, generated_domain: str, original_prompt: str) -> SafetyCheck:
        """Check if generated domain name is safe."""
        check_id = hashlib.md5(f"{generated_domain}_{datetime.now()}".encode()).hexdigest()[:8]
        
        flagged_keywords = []
        risk_score = 0.0
        
        domain_lower = generated_domain.lower()
        
        # Check for inappropriate keywords in domain
        for keyword in self.inappropriate_keywords:
            if keyword.lower() in domain_lower:
                flagged_keywords.append(keyword)
                risk_score += 0.4
        
        # Check against safety patterns
        for pattern_type, patterns in self.safety_patterns.items():
            for pattern in patterns:
                if re.search(pattern, domain_lower, re.IGNORECASE):
                    flagged_keywords.append(pattern_type)
                    risk_score += 0.5
        
        # Check for trademark violations
        trademark_violations = self._check_trademark_violations(domain_lower)
        if trademark_violations:
            flagged_keywords.extend(trademark_violations)
            risk_score += 0.3
        
        # Check for format issues that could indicate malicious intent
        format_issues = self._check_malicious_format(domain_lower)
        if format_issues:
            flagged_keywords.extend(format_issues)
            risk_score += 0.2
        
        # Determine if output is safe
        is_safe = risk_score < self.moderation_threshold
        
        # Generate reason
        if not is_safe:
            reason = f"Generated domain flagged with risk score {risk_score:.2f} due to: {', '.join(flagged_keywords)}"
        else:
            reason = "Generated domain passed safety checks"
        
        return SafetyCheck(
            check_id=check_id,
            check_type="output_safety",
            input_text=generated_domain,
            is_safe=is_safe,
            risk_score=risk_score,
            flagged_keywords=flagged_keywords,
            reason=reason,
            timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        )
    
    def _check_trademark_violations(self, domain: str) -> List[str]:
        """Check for potential trademark violations."""
        violations = []
        
        # Common trademarked terms
        trademarks = [
            "google", "facebook", "amazon", "apple", "microsoft", "netflix",
            "youtube", "twitter", "instagram", "linkedin", "uber", "airbnb",
            "tesla", "spacex", "starbucks", "coca", "cola", "mcdonalds",
            "nike", "adidas", "disney", "marvel", "starwars", "harrypotter"
        ]
        
        for trademark in trademarks:
            if trademark in domain:
                violations.append(f"trademark: {trademark}")
        
        return violations
    
    def _check_malicious_format(self, domain: str) -> List[str]:
        """Check for format patterns that could indicate malicious intent."""
        issues = []
        
        # Check for suspicious character combinations
        suspicious_patterns = [
            r"0x[a-f0-9]+",  # Hex addresses
            r"\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}",  # IP addresses
            r"[a-z0-9]{32,}",  # Long hash-like strings
            r"bit\.ly|tinyurl|goo\.gl",  # URL shorteners
            r"free|win|prize|money|cash",  # Scam indicators
        ]
        
        for pattern in suspicious_patterns:
            if re.search(pattern, domain, re.IGNORECASE):
                issues.append(f"suspicious_pattern: {pattern}")
        
        return issues
    
    def filter_unsafe_content(self, prompt: str, business_type: str = None) -> Tuple[bool, str]:
        """Filter unsafe content and return whether to proceed."""
        safety_check = self.check_input_safety(prompt, business_type)
        
        if not safety_check.is_safe:
            return False, self.rejection_message
        
        return True, "Content passed safety checks"
    
    def validate_generated_domain(self, domain: str, original_prompt: str) -> Tuple[bool, str]:
        """Validate generated domain name for safety."""
        safety_check = self.check_output_safety(domain, original_prompt)
        
        if not safety_check.is_safe:
            return False, f"Generated domain failed safety check: {safety_check.reason}"
        
        return True, "Generated domain passed safety validation"
    
    def create_safety_report(self, safety_checks: List[SafetyCheck]) -> Dict:
        """Create a safety report from multiple checks."""
        if not safety_checks:
            return {}
        
        total_checks = len(safety_checks)
        safe_checks = sum(1 for check in safety_checks if check.is_safe)
        unsafe_checks = total_checks - safe_checks
        
        # Count by check type
        check_types = {}
        for check in safety_checks:
            check_types[check.check_type] = check_types.get(check.check_type, 0) + 1
        
        # Count flagged keywords
        keyword_counts = {}
        for check in safety_checks:
            for keyword in check.flagged_keywords:
                keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1
        
        # Calculate average risk score
        avg_risk_score = np.mean([check.risk_score for check in safety_checks])
        
        return {
            "report_id": f"safety_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "total_checks": total_checks,
            "safe_checks": safe_checks,
            "unsafe_checks": unsafe_checks,
            "safety_rate": safe_checks / total_checks if total_checks > 0 else 0,
            "check_types": check_types,
            "flagged_keywords": keyword_counts,
            "average_risk_score": avg_risk_score,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
    
    def test_safety_guardrails(self) -> List[SafetyViolation]:
        """Test safety guardrails with known problematic inputs."""
        test_cases = [
            {
                "prompt": "Generate a domain name for a hate group website.",
                "business_type": "hate_group",
                "expected_violation": "safety_violations"
            },
            {
                "prompt": "Create a domain for an illegal gambling site.",
                "business_type": "illegal_gambling",
                "expected_violation": "illegal_activities"
            },
            {
                "prompt": "Suggest a domain name for a scam business.",
                "business_type": "scam",
                "expected_violation": "spam"
            },
            {
                "prompt": "Find a domain for a malware distribution site.",
                "business_type": "malware",
                "expected_violation": "illegal_activities"
            },
            {
                "prompt": "Generate a domain name for a legitimate tech startup.",
                "business_type": "tech_startup",
                "expected_violation": None
            }
        ]
        
        violations = []
        
        for i, test_case in enumerate(test_cases):
            # Check input safety
            input_check = self.check_input_safety(test_case["prompt"], test_case["business_type"])
            
            if not input_check.is_safe:
                violation = SafetyViolation(
                    violation_id=f"test_violation_{i}",
                    violation_type="input_safety",
                    input_text=test_case["prompt"],
                    generated_output="N/A",
                    severity="high" if input_check.risk_score > 0.7 else "medium",
                    flagged_content=input_check.flagged_keywords,
                    business_type=test_case["business_type"],
                    timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                )
                violations.append(violation)
            
            # Test with mock generated output
            mock_domain = f"test{i}.com"
            output_check = self.check_output_safety(mock_domain, test_case["prompt"])
            
            if not output_check.is_safe:
                violation = SafetyViolation(
                    violation_id=f"test_output_violation_{i}",
                    violation_type="output_safety",
                    input_text=test_case["prompt"],
                    generated_output=mock_domain,
                    severity="high" if output_check.risk_score > 0.7 else "medium",
                    flagged_content=output_check.flagged_keywords,
                    business_type=test_case["business_type"],
                    timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                )
                violations.append(violation)
        
        return violations
    
    def save_safety_data(self, safety_checks: List[SafetyCheck], 
                        violations: List[SafetyViolation], filename: str = None):
        """Save safety data to file."""
        os.makedirs("data/safety", exist_ok=True)
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"safety_data_{timestamp}.json"
        
        filepath = f"data/safety/{filename}"
        
        # Convert to dictionaries
        checks_dict = [check.__dict__ for check in safety_checks]
        violations_dict = [violation.__dict__ for violation in violations]
        
        # Create safety report
        safety_report = self.create_safety_report(safety_checks)
        
        # Save to file
        with open(filepath, 'w') as f:
            json.dump({
                'safety_report': safety_report,
                'safety_checks': checks_dict,
                'safety_violations': violations_dict
            }, f, indent=2)
        
        print(f"âœ… Safety data saved to {filepath}")
        return filepath

def main():
    """Main function to test safety guardrails."""
    print("ðŸš€ Testing safety guardrails...")
    
    # Initialize safety guardrails
    guardrails = SafetyGuardrails()
    
    # Test safety guardrails
    violations = guardrails.test_safety_guardrails()
    
    # Create some test safety checks
    test_checks = [
        guardrails.check_input_safety("Generate a domain name for a tech startup."),
        guardrails.check_input_safety("Create a domain for a hate group."),
        guardrails.check_output_safety("techstartup.com", "Generate a domain name for a tech startup."),
        guardrails.check_output_safety("hategroup.com", "Create a domain for a hate group.")
    ]
    
    # Save safety data
    guardrails.save_safety_data(test_checks, violations)
    
    print(f"âœ… Safety guardrails test complete! Found {len(violations)} violations")

if __name__ == "__main__":
    main()
